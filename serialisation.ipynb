{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# serialisation formats: pickle, HDF5, h5, hickle, ..."
      ],
      "metadata": {
        "id": "1ORMjhO69i9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dumps"
      ],
      "metadata": {
        "id": "lIBn1BFGGWml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itsdangerous.serializer import Serializer\n",
        "s = Serializer(\"secret-key\")\n",
        "s.dumps([1, 2, 3, 4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "FRQEa3HDGTaO",
        "outputId": "a7281e52-412b-47ea-bda4-cd9786d75b8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[1, 2, 3, 4].r7R9RhGgDPvvWl3iNzLuIIfELmo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loads"
      ],
      "metadata": {
        "id": "PT566l4NGT28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s.loads('[1, 2, 3, 4].r7R9RhGgDPvvWl3iNzLuIIfELmo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrEDLnz5GT92",
        "outputId": "ce241894-9d09-468e-8a52-af82ab87bfb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. pickle format, only in python"
      ],
      "metadata": {
        "id": "-EhpRdbK9o3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        " \n",
        "# A test object\n",
        "test_dict = {\"Hello\": \"World!\"}\n",
        " \n",
        "# Serialization\n",
        "with open(\"test.pickle\", \"wb\") as outfile:\n",
        "    pickle.dump(test_dict, outfile)\n",
        "print(\"Written object\", test_dict)\n",
        " \n",
        "# Deserialization\n",
        "with open(\"test.pickle\", \"rb\") as infile:\n",
        "    test_dict_reconstructed = pickle.load(infile)\n",
        "print(\"Reconstructed object\", test_dict_reconstructed)\n",
        " \n",
        "if test_dict == test_dict_reconstructed:\n",
        "    print(\"Reconstruction success\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soWLIyU5v6U6",
        "outputId": "3bee0b8f-2b0f-4cc9-d58e-e70bed9f1e03"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written object {'Hello': 'World!'}\n",
            "Reconstructed object {'Hello': 'World!'}\n",
            "Reconstruction success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        " \n",
        "class NewClass:\n",
        "    def __init__(self, data):\n",
        "        print(data)\n",
        "        self.data = data\n",
        " \n",
        "# Create an object of NewClass\n",
        "new_class = NewClass(1)\n",
        " \n",
        "# Serialize and deserialize\n",
        "pickled_data = pickle.dumps(new_class)\n",
        "reconstructed = pickle.loads(pickled_data)\n",
        " \n",
        "# Verify\n",
        "print(\"Data from reconstructed object:\", reconstructed.data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUpiIdtlv6aD",
        "outputId": "5e3bc5ac-2c53-4f15-9d34-f5e86bf80470"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Data from reconstructed object: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle and tensorflow"
      ],
      "metadata": {
        "id": "OTrFSpSn902o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        " \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Dropout, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        " \n",
        "# Load MNIST digits\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        " \n",
        "# Reshape data to (n_samples, height, wiedth, n_channel)\n",
        "X_train = np.expand_dims(X_train, axis=3).astype(\"float32\")\n",
        "X_test = np.expand_dims(X_test, axis=3).astype(\"float32\")\n",
        " \n",
        "# One-hot encode the output\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        " \n",
        "# LeNet5 model\n",
        "model = Sequential([\n",
        "    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n",
        "    AveragePooling2D((2,2), strides=2),\n",
        "    Conv2D(16, (5,5), activation=\"tanh\"),\n",
        "    AveragePooling2D((2,2), strides=2),\n",
        "    Conv2D(120, (5,5), activation=\"tanh\"),\n",
        "    Flatten(),\n",
        "    Dense(84, activation=\"tanh\"),\n",
        "    Dense(10, activation=\"softmax\")\n",
        "])\n",
        " \n",
        "# Train the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[earlystopping])\n",
        " \n",
        "# Evaluate the model\n",
        "print(model.evaluate(X_test, y_test, verbose=0))\n",
        " \n",
        "# Pickle to serialize and deserialize\n",
        "pickled_model = pickle.dumps(model)\n",
        "reconstructed = pickle.loads(pickled_model)\n",
        " \n",
        "# Evaluate again\n",
        "print(reconstructed.evaluate(X_test, y_test, verbose=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m50meixc9093",
        "outputId": "5efc8f80-04a0-4c54-d7f2-6eff4b2d9014"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/100\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.1510 - accuracy: 0.9551 - val_loss: 0.0759 - val_accuracy: 0.9739\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0699 - accuracy: 0.9784 - val_loss: 0.0622 - val_accuracy: 0.9807\n",
            "Epoch 3/100\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.0514 - accuracy: 0.9838 - val_loss: 0.0585 - val_accuracy: 0.9827\n",
            "Epoch 4/100\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0419 - accuracy: 0.9869 - val_loss: 0.0493 - val_accuracy: 0.9837\n",
            "Epoch 5/100\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0373 - accuracy: 0.9876 - val_loss: 0.0503 - val_accuracy: 0.9845\n",
            "Epoch 6/100\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0307 - accuracy: 0.9904 - val_loss: 0.0596 - val_accuracy: 0.9812\n",
            "Epoch 7/100\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0267 - accuracy: 0.9915 - val_loss: 0.0503 - val_accuracy: 0.9845\n",
            "Epoch 8/100\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0467 - val_accuracy: 0.9854\n",
            "Epoch 9/100\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0200 - accuracy: 0.9936 - val_loss: 0.0495 - val_accuracy: 0.9857\n",
            "Epoch 10/100\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.0185 - accuracy: 0.9940 - val_loss: 0.0444 - val_accuracy: 0.9870\n",
            "Epoch 11/100\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.0444 - val_accuracy: 0.9874\n",
            "Epoch 12/100\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.0176 - accuracy: 0.9943 - val_loss: 0.0491 - val_accuracy: 0.9865\n",
            "Epoch 13/100\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.0153 - accuracy: 0.9949 - val_loss: 0.0399 - val_accuracy: 0.9887\n",
            "Epoch 14/100\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.0499 - val_accuracy: 0.9861\n",
            "Epoch 15/100\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.0169 - accuracy: 0.9945 - val_loss: 0.0469 - val_accuracy: 0.9869\n",
            "Epoch 16/100\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.0499 - val_accuracy: 0.9871\n",
            "Epoch 17/100\n",
            "1875/1875 [==============================] - 55s 30ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.0467 - val_accuracy: 0.9869\n",
            "[0.039929646998643875, 0.9886999726295471]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.039929646998643875, 0.9886999726295471]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. HDF5 format, name.hdf5"
      ],
      "metadata": {
        "id": "xPN0Ux_Mv6c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIoJaHgc91BQ",
        "outputId": "126a4a67-255d-4ec6-d2e4-ca783a8c485a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py"
      ],
      "metadata": {
        "id": "56CVXtMV91D4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(\"test.hdf5\", \"w\") as file:\n",
        "    # creates new group_1 in file\n",
        "    file.create_group(\"group_1\")\n",
        "    group1 = file[\"group_1\"]\n",
        "    # creates dataset inside group1\n",
        "    group1.create_dataset(\"dataset1\", shape=(10,))\n",
        "    # to access the dataset\n",
        "    dataset = file[\"group_1\"][\"dataset1\"]"
      ],
      "metadata": {
        "id": "6mgHedJI91Gp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. HDF5 Tensorflow format, name.h5"
      ],
      "metadata": {
        "id": "k2ovaOnd_OAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        " \n",
        "# Create model\n",
        "model = keras.models.Sequential([\n",
        " \tkeras.layers.Input(shape=(10,)),\n",
        " \tkeras.layers.Dense(1)\n",
        "])\n",
        " \n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        " \n",
        "# using the .h5 extension in the file name specifies that the model\n",
        "# should be saved in HDF5 format\n",
        "model.save(\"my_model.h5\")"
      ],
      "metadata": {
        "id": "atypTxGJ_ODf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        " \n",
        "with h5py.File(\"my_model.h5\", \"r\") as infile:\n",
        "    for key in infile.attrs.keys():\n",
        "        formatted = infile.attrs[key]\n",
        "        if key.endswith(\"_config\"):\n",
        "            formatted = json.dumps(json.loads(formatted), indent=4)\n",
        "        print(f\"{key}: {formatted}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT5aSFOU_OHc",
        "outputId": "7b533992-1e06-423c-cb1b-bde642370ffb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "backend: tensorflow\n",
            "keras_version: 2.9.0\n",
            "model_config: {\n",
            "    \"class_name\": \"Sequential\",\n",
            "    \"config\": {\n",
            "        \"name\": \"sequential_1\",\n",
            "        \"layers\": [\n",
            "            {\n",
            "                \"class_name\": \"InputLayer\",\n",
            "                \"config\": {\n",
            "                    \"batch_input_shape\": [\n",
            "                        null,\n",
            "                        10\n",
            "                    ],\n",
            "                    \"dtype\": \"float32\",\n",
            "                    \"sparse\": false,\n",
            "                    \"ragged\": false,\n",
            "                    \"name\": \"input_1\"\n",
            "                }\n",
            "            },\n",
            "            {\n",
            "                \"class_name\": \"Dense\",\n",
            "                \"config\": {\n",
            "                    \"name\": \"dense_2\",\n",
            "                    \"trainable\": true,\n",
            "                    \"dtype\": \"float32\",\n",
            "                    \"units\": 1,\n",
            "                    \"activation\": \"linear\",\n",
            "                    \"use_bias\": true,\n",
            "                    \"kernel_initializer\": {\n",
            "                        \"class_name\": \"GlorotUniform\",\n",
            "                        \"config\": {\n",
            "                            \"seed\": null\n",
            "                        }\n",
            "                    },\n",
            "                    \"bias_initializer\": {\n",
            "                        \"class_name\": \"Zeros\",\n",
            "                        \"config\": {}\n",
            "                    },\n",
            "                    \"kernel_regularizer\": null,\n",
            "                    \"bias_regularizer\": null,\n",
            "                    \"activity_regularizer\": null,\n",
            "                    \"kernel_constraint\": null,\n",
            "                    \"bias_constraint\": null\n",
            "                }\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "}\n",
            "training_config: {\n",
            "    \"loss\": \"mse\",\n",
            "    \"metrics\": null,\n",
            "    \"weighted_metrics\": null,\n",
            "    \"loss_weights\": null,\n",
            "    \"optimizer_config\": {\n",
            "        \"class_name\": \"Adam\",\n",
            "        \"config\": {\n",
            "            \"name\": \"Adam\",\n",
            "            \"learning_rate\": 0.001,\n",
            "            \"decay\": 0.0,\n",
            "            \"beta_1\": 0.9,\n",
            "            \"beta_2\": 0.999,\n",
            "            \"epsilon\": 1e-07,\n",
            "            \"amsgrad\": false\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. hickle"
      ],
      "metadata": {
        "id": "WThNN9jKB4k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfOV9rvTCLzf",
        "outputId": "6fb3e72b-1dea-4aa7-a4db-0daa07958095"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hickle\n",
            "  Downloading hickle-5.0.2-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy!=1.20,>=1.8 in /usr/local/lib/python3.7/dist-packages (from hickle) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->hickle) (1.5.2)\n",
            "Installing collected packages: hickle\n",
            "Successfully installed hickle-5.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hickle as hkl\n",
        "import numpy as np\n",
        "\n",
        "# Create a numpy array of data\n",
        "array_obj = np.ones(32768, dtype='float32')\n",
        "\n",
        "# Dump to file\n",
        "hkl.dump(array_obj, 'test.hkl', mode='w')\n",
        "\n",
        "# Dump data, with compression\n",
        "hkl.dump(array_obj, 'test_gzip.hkl', mode='w', compression='gzip')\n",
        "\n",
        "# Compare filesizes\n",
        "print('uncompressed: %i bytes' % os.path.getsize('test.hkl'))\n",
        "print('compressed:   %i bytes' % os.path.getsize('test_gzip.hkl'))\n",
        "\n",
        "# Load data\n",
        "array_hkl = hkl.load('test_gzip.hkl')\n",
        "\n",
        "# Check the two are the same file\n",
        "assert array_hkl.dtype == array_obj.dtype\n",
        "assert np.all((array_hkl, array_obj))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzxBLYv0B5Hp",
        "outputId": "05937cb7-c422-4586-e4ed-54fb59aa2d02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uncompressed: 139284 bytes\n",
            "compressed:   11988 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. cpickle"
      ],
      "metadata": {
        "id": "28Q3ehxXB5PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import cPickle as pickle\n",
        "except:\n",
        "    import pickle\n",
        "import sys\n",
        "\n",
        "class SimpleObject(object):\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        l = list(name)\n",
        "        l.reverse()\n",
        "        self.name_backwards = ''.join(l)\n",
        "        return"
      ],
      "metadata": {
        "id": "KDi92Uz0EIcN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. HDF5 Pandas format, name.h5"
      ],
      "metadata": {
        "id": "PpL-JIH5EVAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  \n",
        "df.to_hdf('./store.h5', 'data')  \n",
        "reread = pd.read_hdf('./store.h5') "
      ],
      "metadata": {
        "id": "Ogmoho_OHEyj"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}